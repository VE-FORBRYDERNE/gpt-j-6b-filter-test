{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "infer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APNYUdyRYAz1"
      },
      "source": [
        "# Press CTRL+F9 to run everything\n",
        "This is based on the [original notebook](http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb) from the authors of GPT-J-6B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7xAFw-LOYfe"
      },
      "source": [
        "!apt install zstd\n",
        "!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
        "!time tar -I zstd -xf step_383500_slim.tar.zstd\n",
        "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
        "!pip install -r mesh-transformer-jax/requirements.txt\n",
        "!pip install mesh-transformer-jax/ jax==0.2.12 tensorflow==2.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTrGiIXMtafA"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''<style>pre {white-space: pre-wrap;}</style>'''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAgKq-X2kmba"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from jax.config import config\n",
        "\n",
        "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
        "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
        "requests.post(url)\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "import time\n",
        "import jax\n",
        "from jax.experimental import maps\n",
        "import numpy as np\n",
        "import optax\n",
        "import transformers\n",
        "from mesh_transformer.checkpoint import read_ckpt\n",
        "from mesh_transformer.sampling import nucleaus_sample\n",
        "from mesh_transformer.transformer_shard import CausalTransformer\n",
        "\n",
        "import random\n",
        "import haiku as hk\n",
        "import jax.numpy as jnp\n",
        "from mesh_transformer.transformer_shard import CausalTransformerShard\n",
        "class PenalizingCausalTransformer(CausalTransformer):\n",
        "    '''\n",
        "    This is a custom version of CausalTransformer I made that supports the\n",
        "    repetition penalty described in this paper:\n",
        "    https://arxiv.org/pdf/1909.05858.pdf\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        def generate(state, key, ctx, ctx_length, aux, sampler_options):\n",
        "            sampler = config[\"sampler\"]\n",
        "            gen_length = self.gen_length\n",
        "\n",
        "            self.endoftext_id = 50256\n",
        "\n",
        "            def generate_sample(context, ctx_length, aux):\n",
        "                transformer = CausalTransformerShard(config)\n",
        "                _, initial_state = transformer.generate_initial(context, ctx_length)\n",
        "\n",
        "                generated_range = jnp.arange(config[\"seq\"])\n",
        "                generated_mask = jnp.asarray(generated_range < ctx_length)[::-1]\n",
        "                generated = jnp.where(generated_mask, context, 50256)\n",
        "                generated = jnp.pad(generated, (0, gen_length), constant_values=50256)\n",
        "                generated = jnp.tile(generated, (self.batch_size, 1))\n",
        "                generated_index = config[\"seq\"]\n",
        "\n",
        "                initial_state = (generated, generated_index) + initial_state\n",
        "                repetition_penalty = sampler_options.pop('repetition_penalty', None)\n",
        "\n",
        "                def apply_penalty_2d(logits, tokens_2d, repetition_penalty):\n",
        "                    shift = jnp.reshape(jnp.repeat(jnp.arange(tokens_2d.shape[0]) * logits.shape[1], tokens_2d.shape[1]), tokens_2d.shape)\n",
        "                    penalty_logits = jnp.take(logits, tokens_2d + shift)\n",
        "                    penalty_logits = jnp.where(penalty_logits > 0, penalty_logits/repetition_penalty, penalty_logits*repetition_penalty)\n",
        "                    return logits.at[(jnp.repeat(jnp.arange(penalty_logits.shape[0]), penalty_logits.shape[1]), tokens_2d.flatten())].set(penalty_logits.flatten())\n",
        "\n",
        "                def generate_scan_fn(carry, sampler_input):\n",
        "                    generated, generated_index, next_token, decode_state, sample_key = carry\n",
        "                    sample_key, new_key = jax.random.split(sample_key)\n",
        "\n",
        "                    logits, new_state = transformer.generate_once(next_token, decode_state)\n",
        "\n",
        "                    # Apply repetition penalty to tokens that have already\n",
        "                    # appeared in the context or in tokens previously chosen\n",
        "                    # by sampler() in this run of generate_sample()\n",
        "                    if repetition_penalty is not None:\n",
        "                        logits = apply_penalty_2d(logits, generated, repetition_penalty)\n",
        "\n",
        "                    # Prevent <|endoftext|> from appearing in the output by\n",
        "                    # setting its logit value to negative infinity\n",
        "                    logits = logits.at[:, (50256,)].set(-jnp.inf)\n",
        "\n",
        "                    next_token, sample_info = sampler(sample_key, logits, sampler_input, **sampler_options)\n",
        "\n",
        "                    generated = generated.at[:, generated_index].set(next_token.flatten())\n",
        "                    generated_index += 1\n",
        "\n",
        "                    if self.return_logits:\n",
        "                        output = (next_token, sample_info, logits)\n",
        "                    else:\n",
        "                        output = (next_token, sample_info)\n",
        "                    new_carry = (generated, generated_index, next_token, new_state, new_key)\n",
        "                    return new_carry, output\n",
        "\n",
        "                final_state, outputs = jax.lax.scan(generate_scan_fn, initial_state, xs=aux, length=gen_length)\n",
        "                return final_state, outputs\n",
        "\n",
        "            generate_fn = hk.transform(generate_sample).apply\n",
        "            return generate_fn(state[\"params\"], key, ctx, ctx_length, aux)\n",
        "\n",
        "        self.generate_xmap = jax.experimental.maps.xmap(fun=generate,\n",
        "                                                        in_axes=([\"shard\", ...],\n",
        "                                                                 [\"batch\", ...],\n",
        "                                                                 [\"batch\", ...],\n",
        "                                                                 [\"batch\", ...],\n",
        "                                                                 [\"batch\", ...],\n",
        "                                                                 [\"batch\", ...]),\n",
        "                                                        out_axes=[\"batch\", ...],\n",
        "                                                        axis_resources={'shard': 'mp', 'batch': 'dp'})\n",
        "\n",
        "    def generate(self, ctx, ctx_length, batch_size, gen_length, sampler_options, return_logits=False):\n",
        "        key = hk.PRNGSequence(random.randint(0, 2 ** 60))\n",
        "\n",
        "        batch_size = ctx.shape[0]\n",
        "        aux = jnp.zeros((batch_size, gen_length), dtype=jnp.uint32)\n",
        "        self.gen_length = gen_length\n",
        "        self.batch_size = batch_size\n",
        "        self.return_logits = return_logits\n",
        "\n",
        "        return self.generate_xmap(self.state,\n",
        "                                  jnp.array(key.take(batch_size)),\n",
        "                                  ctx,\n",
        "                                  np.array(ctx_length, dtype=np.uint32),\n",
        "                                  aux,\n",
        "                                  sampler_options)\n",
        "\n",
        "params = {\n",
        "  \"layers\": 28,\n",
        "  \"d_model\": 4096,\n",
        "  \"n_heads\": 16,\n",
        "  \"n_vocab\": 50400,\n",
        "  \"norm\": \"layernorm\",\n",
        "  \"pe\": \"rotary\",\n",
        "  \"pe_rotary_dims\": 64,\n",
        "\n",
        "  \"seq\": 2048,\n",
        "  \"cores_per_replica\": 8,\n",
        "  \"per_replica_batch\": 1,\n",
        "}\n",
        "\n",
        "per_replica_batch = params[\"per_replica_batch\"]\n",
        "cores_per_replica = params[\"cores_per_replica\"]\n",
        "seq = params[\"seq\"]\n",
        "\n",
        "\n",
        "params[\"sampler\"] = nucleaus_sample\n",
        "params[\"optimizer\"] = optax.scale(0)\n",
        "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
        "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
        "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
        "network = PenalizingCausalTransformer(params)\n",
        "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
        "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def infer(context, top_p=0.9, temp=0.5, repetition_penalty=1.2, gen_len=200):\n",
        "    tokens = tokenizer.encode(context, max_length=params[\"seq\"], truncation=True)\n",
        "    provided_ctx = len(tokens)\n",
        "    pad_amount = seq - provided_ctx\n",
        "    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "    batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "    output = network.generate(batched_tokens, length, total_batch, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp, \"repetition_penalty\": np.ones(total_batch) * repetition_penalty})\n",
        "    samples = []\n",
        "    decoded_tokens = output[1][0]\n",
        "    for o in decoded_tokens[:, :, 0]:\n",
        "        samples.append(tokenizer.decode(o))\n",
        "    return samples\n",
        " \n",
        "# We need to pre-run this function once because the first run takes ~1 minute for compilation\n",
        "context = \"Hey guys! First, I'd like to give a big shout-out to Squarespace for making all of this possible.\"\n",
        "print(f\"\\033[1m{context}\", end='')\n",
        "print(f\"\\033[0m{infer(context)[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlAK6RbCJYg"
      },
      "source": [
        "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "temp = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "repetition_penalty = 1.2 #@param {type:\"slider\", min:1, max:1.3, step:0.005}\n",
        "\n",
        "context = \"\"\"My name is Yoshikage Kira.\"\"\"\n",
        "\n",
        "print(f\"\\033[1m{context}\", end='')\n",
        "print(f\"\\033[0m{infer(top_p=top_p, temp=temp, repetition_penalty=repetition_penalty, gen_len=200, context=context)[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9f68UdW009N"
      },
      "source": [
        "## What do the sliders do? Here's a quick explanation.\n",
        "\n",
        "* `temp` (temperature) is a positive real number. It makes the output more creative and spontaneous the larger it is. Low values make the output repetitive. Setting this higher than 1 is a bad idea.\n",
        "* `top_p` (top-_p_) is a real number inclusively between 0 and 1. The lower you set it, the more the program tries to remove incongruous words and punctuation from the output. Setting this to 1 disables the effect. Setting this to exactly 0 makes the program always output the exact same thing for the same input parameters (i.e. makes it deterministic). Most people recommend setting this to somewhere in the 0.9-1.0 (inclusive) range. Setting this lower can help it answer knowledge-based questions better but makes it very bad at creative writing.\n",
        "* `repetition_penalty` is a positive real number. If it's greater than 1, words that have already appeared are discouraged from appearing again; if it's less than 1 (don't do this) they're encouraged instead. Setting this to 1 disables the effect. It is recommended to set this to 1.2. Setting this higher than 1.2 can have disastrous results.\n",
        "\n",
        "---\n",
        "\n",
        "The language model works by reading through the `context` and assigning a \"score\" between 0 and 1 (where 0 is weakest and 1 is strongest) to each of the 50257 possible tokens in its vocabulary. A \"token\" is a particular sequence of characters, each one assigned an integer from 0 to 50256. We get the logits of those scores (\"logit\" is a mathematical function used in neural networks that maps real numbers between 0 and 1 exclusive to the entire real number line).\n",
        "\n",
        "This JAX program works by sending the context to the language model and getting the logits, picking one token to append to the context, sending the new context back to the language model, and so on, the number of times specified by `gen_length`. The question is then how exactly we choose the token to append to the context based on the logits from the language model.\n",
        "\n",
        "The easy way is just to pick the token with the greatest logit; this is called greedy sampling. You can enable greedy sampling by setting `top_p` to 0. Greedy sampling is good at answering questions but inferior at generating creative text.\n",
        "\n",
        "Nucleus sampling works by first dividing the logit values by `temp`, sorting the tokens from greatest to least logit, removing some of the tokens with the least logits, then using the softmax function to pick one of the remaining ones (higher logit equals higher probability). Specifically, we remove some tokens by sorting the tokens from greatest to least logit, then calculating the cumulative probability of each token being chosen (i.e. the probability that this token or any tokens with a higher logit value are chosen), then tokens with a cumulative probability higher than `top_p` are removed. Setting `top_p` to 0 only keeps the token with the highest logit value making it equivalent to greedy sampling. The effect that dividing the logits by `temp` has is, if `temp` is greater than 1, it moves the logit values closer to to each other, and if less than 1 it moves them further away from each other. This results in the token choosing probabilities becoming more similar or more different.\n",
        "\n",
        "I added in a method called penalized sampling, the original version of which was described in section 4.1 (pages 4-5) of this paper: https://arxiv.org/pdf/1909.05858.pdf. Tokens that have already appeared in the context (including tokens the program chose and added to the initial context) have their probabilities of being chosen artificially lowered by dividing their logit values by `repetition_penalty` if positive or multiplying if negative. The paper suggests 1.2 as a good value for `repetition_penalty`."
      ]
    }
  ]
}
